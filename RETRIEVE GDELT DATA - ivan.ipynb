{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20181031.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20181030.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20181029.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20181028.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20181027.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20181026.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20181025.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20181024.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20181023.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20181022.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20181021.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20181020.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20181019.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20181018.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20181017.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20181016.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20181015.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20181014.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20181013.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20181012.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20181011.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20181010.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20181009.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20181008.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20181007.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20181006.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20181005.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20181004.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20181003.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20181002.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20181001.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180930.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180929.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180928.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180927.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180926.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180925.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180924.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180923.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180922.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180921.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180920.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180919.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180918.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180917.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180916.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180915.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180914.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180913.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180912.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180911.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180910.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180909.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180908.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180907.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180906.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180905.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180904.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180903.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180902.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180901.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180831.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180830.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180829.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180828.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180827.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180826.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180825.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180824.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180823.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180822.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180821.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180820.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180819.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180818.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180817.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180816.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180815.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180814.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180813.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180812.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180811.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180810.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180809.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180808.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180807.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180806.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180805.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180804.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180803.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180802.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n",
      "20180801.export.CSV.zip\n",
      "extracting,\n",
      "parsing,\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import lxml.html as lh\n",
    " \n",
    "gdelt_base_url = 'http://data.gdeltproject.org/events/'\n",
    "\n",
    "# get the list of all the links on the gdelt file page\n",
    "page = requests.get(gdelt_base_url+'index.html')\n",
    "# print(page)\n",
    "doc = lh.fromstring(page.content)\n",
    "# print(type(doc))\n",
    "link_list = doc.xpath(\"//*/ul/li/a/@href\")\n",
    "# print(link_list)\n",
    "\n",
    "file_list = [x for x in link_list if str.isdigit(x[0:4]) and int(x[0:4])>2005 if 201808<=int(x[0:6])<=201810] #ambil tahun 2018 3 bln ajah\n",
    "# print(file_list)\n",
    "\n",
    "infilecounter = 0\n",
    "outfilecounter = 0\n",
    "\n",
    "import os.path\n",
    "import urllib.request as urllib\n",
    "import zipfile\n",
    "import glob\n",
    "import operator\n",
    "\n",
    "local_path = 'D:/Digitalent Kominfo 2018/GDELT/'\n",
    "\n",
    "fips_country_code = 'ID'\n",
    "\n",
    "# for compressed_file in file_list[1:2]:\n",
    "for compressed_file in file_list[infilecounter:]:\n",
    "    print(compressed_file)\n",
    "    while not os.path.isfile(local_path + compressed_file):\n",
    "        print('downloading, '),\n",
    "        urllib.urlretrieve(url=gdelt_base_url + compressed_file,\n",
    "                           filename=local_path + compressed_file)\n",
    "        \n",
    "    # extract the contents of the compressed file to a temporary directory\n",
    "    print('extracting,'),\n",
    "    z = zipfile.ZipFile(file=local_path + compressed_file, mode='r')\n",
    "    z.extractall(path=local_path + 'tmp/')\n",
    "    \n",
    "    # parse each of the csv files in the working directory,\n",
    "    print('parsing,'),\n",
    "    for infile_name in glob.glob(local_path + 'tmp/*'):\n",
    "        outfile_name = local_path + 'country/' + fips_country_code + '%04i.tsv' % outfilecounter\n",
    "\n",
    "        # open the infile and outfile\n",
    "        with open(infile_name, mode='r', encoding='latin1') as infile, open(outfile_name, mode='w', encoding='latin1') as outfile:\n",
    "            outfile.write('GLOBALEVENTID\tSQLDATE\tMonthYear\tYear\tFractionDate\tActor1Code\tActor1Name\tActor1CountryCode\tActor1KnownGroupCode\tActor1EthnicCode\tActor1Religion1Code\tActor1Religion2Code\tActor1Type1Code\tActor1Type2Code\tActor1Type3Code\tActor2Code\tActor2Name\tActor2CountryCode\tActor2KnownGroupCode\tActor2EthnicCode\tActor2Religion1Code\tActor2Religion2Code\tActor2Type1Code\tActor2Type2Code\tActor2Type3Code\tIsRootEvent\tEventCode\tEventBaseCode\tEventRootCode\tQuadClass\tGoldsteinScale\tNumMentions\tNumSources\tNumArticles\tAvgTone\tActor1Geo_Type\tActor1Geo_FullName\tActor1Geo_CountryCode\tActor1Geo_ADM1Code\tActor1Geo_Lat\tActor1Geo_Long\tActor1Geo_FeatureID\tActor2Geo_Type\tActor2Geo_FullName\tActor2Geo_CountryCode\tActor2Geo_ADM1Code\tActor2Geo_Lat\tActor2Geo_Long\tActor2Geo_FeatureID\tActionGeo_Type\tActionGeo_FullName\tActionGeo_CountryCode\tActionGeo_ADM1Code\tActionGeo_Lat\tActionGeo_Long\tActionGeo_FeatureID\tDATEADDED\tSOURCEURL\\n')\n",
    "            for line in infile:\n",
    "#                 print(line.split('\\t'))\n",
    "#                 # extract lines with our interest country code\n",
    "                if fips_country_code in operator.itemgetter(51, 37, 44)(line.split('\\t')):\n",
    "                    outfile.write(line)\n",
    "            outfilecounter += 1\n",
    "\n",
    "        # delete the temporary file\n",
    "        os.remove(infile_name)\n",
    "        \n",
    "    infilecounter += 1\n",
    "    print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "host = 'localhost'\n",
    "port = \"3306\"\n",
    "username = 'root'\n",
    "password = ''\n",
    "database = 'gdelt'\n",
    "\n",
    "engine=create_engine('mysql+pymysql://'+username+':'+password+'@'+host+':'+port+'/'+database+'?charset=latin1')\n",
    "# create_engine('mysql+pymysql://root:@localhost:3306/classicmodels')\n",
    "\n",
    "def run(sql):\n",
    "    df=pd.read_sql_query(sql, engine)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0000.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0001.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0002.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0003.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0004.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0005.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0006.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0007.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0008.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0009.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0010.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0011.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0012.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0013.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0014.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0015.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0016.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0017.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0018.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0019.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0020.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0021.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0022.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0023.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0024.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0025.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0026.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0027.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0028.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0029.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0030.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0031.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0032.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0033.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0034.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0035.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0036.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0037.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0038.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0039.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0040.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0041.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0042.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0043.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0044.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0045.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0046.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0047.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0048.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0049.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0050.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0051.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0052.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0053.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0054.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0055.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0056.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0057.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0058.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0059.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0060.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0061.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0062.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0063.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0064.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0065.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0066.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0067.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0068.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0069.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0070.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0071.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0072.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0073.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0074.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0075.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0076.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0077.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0078.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0079.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0080.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0081.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0082.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0083.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0084.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0085.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0086.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0087.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0088.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0089.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0090.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0091.tsv\n",
      "D:/Digitalent Kominfo 2018/GDELT/country\\ID0092.tsv\n"
     ]
    }
   ],
   "source": [
    "for countryfile in glob.glob(local_path+'country/*'):\n",
    "    print(countryfile)\n",
    "    df=pd.read_csv(countryfile,delimiter='\\t',encoding='latin1', header=0)\n",
    "    df.to_sql('gdelt_content',con=engine,if_exists='append',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-f C:\\Users\\ivan\\AppData\\Roaming\\jupyter\\runtime\\kernel-f5757c64-f775-477d-b95e-69bebd371864.json\n",
      "SELECT * FROM `gdelt_content` WHERE (TITLE IS NULL OR TRIM(TITLE) = '') limit 10\n",
      "http://www.keranews.org/post/myanmars-military-leaders-should-be-tried-genocide-un-investigators-say\n",
      "http://www.keranews.org/post/myanmars-military-leaders-should-be-tried-genocide-un-investigators-say\n",
      "http://www.keranews.org/post/myanmars-military-leaders-should-be-tried-genocide-un-investigators-say\n",
      "http://www.keranews.org/post/myanmars-military-leaders-should-be-tried-genocide-un-investigators-say\n",
      "http://www.keranews.org/post/myanmars-military-leaders-should-be-tried-genocide-un-investigators-say\n",
      "http://www.keranews.org/post/myanmars-military-leaders-should-be-tried-genocide-un-investigators-say\n",
      "https://www.onenewspage.com/n/Politics/1zj9d2cd09/How-Recent-Shootings-In-Florida-Are-Affecting-Politics.htm\n",
      "https://www.cqnews.com.au/news/local-takes-asylum-seekers-crabbing-after-he-finds/3504944/\n",
      "https://www.cqnews.com.au/news/local-takes-asylum-seekers-crabbing-after-he-finds/3504944/\n",
      "https://indiankanoon.org/doc/193904767/\n"
     ]
    }
   ],
   "source": [
    "from newspaper import Article\n",
    "import pymysql.cursors\n",
    "import sys\n",
    "\n",
    "offset = sys.argv[1]\n",
    "limit = sys.argv[2]\n",
    "# year = sys.argv[3]\n",
    "print(offset,limit)\n",
    "\n",
    "# Connect to the database\n",
    "connection = pymysql.connect(host='localhost',\n",
    "                             user='root',\n",
    "                             password='',\n",
    "                             db='gdelt',\n",
    "                             charset='utf8mb4',\n",
    "                             cursorclass=pymysql.cursors.DictCursor)\n",
    "\n",
    "hdr = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "with connection.cursor() as cursor:\n",
    "#     sql = \"SELECT * FROM `gdelt_content` WHERE (TITLE IS NULL OR TRIM(TITLE) = '') LIMIT \"+str(offset)+\",\"+str(limit)\n",
    "    sql = \"SELECT * FROM `gdelt_content` WHERE (TITLE IS NULL OR TRIM(TITLE) = '') limit 10\"\n",
    "    cursor.execute(sql)\n",
    "    print(sql)\n",
    "    result = cursor.fetchall()\n",
    "    for r in result:\n",
    "        try:\n",
    "#             r[\"SOURCEURL\"] = r[\"SOURCEURL\"].split('<UDIV>')\n",
    "            print(r[\"SOURCEURL\"])\n",
    "            article = Article(r[\"SOURCEURL\"])\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            with connection.cursor() as cursor:\n",
    "                sql = \"UPDATE `gdelt_content` SET title=%s,content=%s WHERE GLOBALEVENTID=%s\"\n",
    "                cursor.execute(sql, (article.title.encode('utf-8'), article.text.encode('utf-8'),r[\"GLOBALEVENTID\"]))\n",
    "                connection.commit()\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DI BAWAH ASLI CODE PAK GUNTUR!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download\n",
    "\n",
    "Berikut adalah contoh script untuk mengunduh data GDELT dengan country code ID. Selanjutnya dari csv bisa dipindah ke mysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import lxml.html as lh\n",
    "\n",
    "gdelt_base_url = 'http://data.gdeltproject.org/events/'\n",
    "\n",
    "# get the list of all the links on the gdelt file page\n",
    "page = requests.get(gdelt_base_url+'index.html')\n",
    "doc = lh.fromstring(page.content)\n",
    "link_list = doc.xpath(\"//*/ul/li/a/@href\")\n",
    "\n",
    "# separate out those links that begin with four digits\n",
    "file_list = [x for x in link_list if str.isdigit(x[0:4])]\n",
    "print(file_list)\n",
    "\n",
    "infilecounter = 0\n",
    "outfilecounter = 0\n",
    "\n",
    "import os.path\n",
    "import urllib\n",
    "import zipfile\n",
    "import glob\n",
    "import operator\n",
    "\n",
    "local_path = 'E:/GDELT/'\n",
    "\n",
    "fips_country_code = 'ID'\n",
    "\n",
    "for compressed_file in file_list[infilecounter:]:\n",
    "    print(compressed_file)\n",
    "\n",
    "    # if we dont have the compressed file stored locally, go get it. Keep trying if necessary.\n",
    "    while not os.path.isfile(local_path + compressed_file):\n",
    "        print('downloading, '),\n",
    "        urllib.urlretrieve(url=gdelt_base_url + compressed_file,\n",
    "                           filename=local_path + compressed_file)\n",
    "\n",
    "    # extract the contents of the compressed file to a temporary directory\n",
    "    print('extracting,'),\n",
    "    z = zipfile.ZipFile(file=local_path + compressed_file, mode='r')\n",
    "    z.extractall(path=local_path + 'tmp/')\n",
    "\n",
    "    # parse each of the csv files in the working directory,\n",
    "    print('parsing,'),\n",
    "    for infile_name in glob.glob(local_path + 'tmp/*'):\n",
    "        outfile_name = local_path + 'country/' + fips_country_code + '%04i.tsv' % outfilecounter\n",
    "\n",
    "        # open the infile and outfile\n",
    "        with open(infile_name, mode='r') as infile, open(outfile_name, mode='w') as outfile:\n",
    "            for line in infile:\n",
    "                # extract lines with our interest country code\n",
    "                if fips_country_code in operator.itemgetter(51, 37, 44)(line.split('\\t')):\n",
    "                    outfile.write(line)\n",
    "            outfilecounter += 1\n",
    "\n",
    "        # delete the temporary file\n",
    "        os.remove(infile_name)\n",
    "    infilecounter += 1\n",
    "    print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping GDELT\n",
    "Berikut adalah kode contoh untuk melakukan scraping dengan library newspaper. Skrip ini akan berjalan jika data dari GDELT telah tersimpan ke MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article\n",
    "import pymysql.cursors\n",
    "import sys\n",
    "\n",
    "offset = sys.argv[1]\n",
    "limit = sys.argv[2]\n",
    "# year = sys.argv[3]\n",
    "\n",
    "# Connect to the database\n",
    "connection = pymysql.connect(host='localhost',\n",
    "                             user='root',\n",
    "                             password='',\n",
    "                             db='news',\n",
    "                             charset='utf8mb4',\n",
    "                             cursorclass=pymysql.cursors.DictCursor)\n",
    "\n",
    "hdr = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "with connection.cursor() as cursor:\n",
    "    sql = \"SELECT * FROM `gdelt_content` WHERE (TITLE IS NULL OR TRIM(TITLE) = '') LIMIT \"+str(offset)+\",\"+str(limit)\n",
    "    cursor.execute(sql)\n",
    "    print(sql)\n",
    "    result = cursor.fetchall()\n",
    "    for r in result:\n",
    "        try:\n",
    "            r[\"URL\"] = r[\"URL\"].split('<UDIV>')\n",
    "            print(r[\"URL\"][0])\n",
    "            article = Article(r[\"URL\"][0])\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            with connection.cursor() as cursor:\n",
    "                sql = \"UPDATE `gdelt_content` SET title=%s,content=%s WHERE GLOBALEVENTID=%s\"\n",
    "                cursor.execute(sql, (article.title.encode('utf-8'), article.text.encode('utf-8'),r[\"GLOBALEVENTID\"]))\n",
    "                connection.commit()\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berikut adalah contoh untuk melakukan pickling data GDELT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "local_path = 'E:/GDELT/'\n",
    "fips_country_code = 'ID'\n",
    "# Get the GDELT field names from a helper file\n",
    "colnames = pd.read_excel('CSV.header.fieldids.xlsx', sheetname='Sheet1',\n",
    "                         index_col='Column ID', parse_cols=1)['Field Name']\n",
    "\n",
    "# Build DataFrames from each of the intermediary files\n",
    "files = glob.glob(local_path + 'country/' + fips_country_code + '*')\n",
    "DFlist = []\n",
    "for active_file in files:\n",
    "    print active_file\n",
    "    DFlist.append(pd.read_csv(active_file, sep='\\t', header=None, dtype=str,\n",
    "                              names=colnames, index_col=['GLOBALEVENTID']))\n",
    "    break\n",
    "\n",
    "# Merge the file-based dataframes and save a pickle\n",
    "DF = pd.concat(DFlist)\n",
    "DF.to_pickle(local_path + 'backupsmall' + fips_country_code + '.pickle')\n",
    "\n",
    "# once everythin is safely stored away, remove the temporary files\n",
    "# for active_file in files:\n",
    "#     os.remove(active_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
